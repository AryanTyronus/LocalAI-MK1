model:
  default: mlx-community/Qwen2.5-7B-Instruct-4bit
  max_tokens: 2048
  context_window: 4096
  threading:
    enabled: true
    max_threads: 4

personality:
  tone: "calm_controlled_witty"
  persistent_prompt: |
    You are LocalAI.
    Tone requirements:
    - Calm and controlled under all conditions.
    - Slightly witty when natural, never playful to the point of distraction.
    - Concise, practical, and direct.
    - Consistent voice across all responses.

ui:
  projects:
    - LocalAI
    - Study
    - Coding

# ============================================
# TOKEN BUDGET CONFIGURATION (Phase 2)
# ============================================
token_budget:
  max_context_tokens: 4096        # Maximum tokens for context (excluding generation)
  reserve_for_generation: 2048    # Reserve tokens for generation output
  enable_trimming: true           # Enable automatic token trimming
  estimation_method: "chars"      # Token estimation: "chars" or "accurate"

# ============================================
# DEBUG & ERROR HANDLING
# ============================================
debug:
  enabled: true  # Set to true to return full error details in API responses
  print_stack_traces: true  # Always print stack traces to console

# ============================================
# 4-LAYER MEMORY ARCHITECTURE CONFIGURATION
# ============================================

memory:
  # Layer 1: Short-term memory (active conversation)
  short_term:
    max_messages: 10        # Keep last 10 messages in active window
    max_tokens: 900         # Token-aware truncation target for short-term context
    enabled: true

  # Layer 2: Rolling summary memory (compressed conversation history)
  rolling_summary:
    enabled: true
    trigger_threshold: 15   # Summarize when messages exceed this
    summary_size: 3         # Keep 3 most recent summaries
    preserve_facts: true    # Preserve key facts, decisions, topics

  # Layer 3: Long-term semantic memory (vector embeddings with search)
  semantic:
    enabled: true
    embedding_model: "all-MiniLM-L6-v2"  # Lightweight embedding model
    index_type: "faiss"     # Vector index backend
    similarity_threshold: 0.5
    max_results: 3          # Maximum memories to retrieve per query
    persist_interval: 10    # Save index every N updates
    max_entries: 2000       # Prevent vector memory growth overflow
    namespace: "auto"       # Per-project namespace (auto = cwd project name)

  # Layer 4: Structured persistent memory (user profile & state)
  structured:
    enabled: true
    file: "memory/long_term.json"
    auto_extract_profile: true  # Auto-extract user info
    categories:
      - user_profile        # Name, age, preferences
      - long_term_goals     # Goals and aspirations
      - preferences         # Communication style, interests
      - system_state        # Important system variables

# Memory management
  thresholds:
    max_total_memory_mb: 500      # Total memory budget
    cleanup_trigger: 0.85         # Cleanup when 85% full
    semantic_memory_batch: 50     # Batch size for indexing

# Summarization settings
summarization:
  model_type: "extractive"  # or "abstractive" (requires larger model)
  key_phrases_count: 5      # Extract N key phrases per summary
  preserve_unresolved: true  # Keep unresolved questions

search:
  default_top_k: 3
  include_summaries: true   # Include summary memory in searches
  include_semantic: true    # Include semantic memory in searches

# Mode configurations for different pipelines
modes:
  chat:
    system_prompt: |
      You are a helpful, friendly assistant. Answer concisely and helpfully.
    temperature: 0.7
    retrieve_documents: true
    memory_profile: short_term_heavy

  coding:
    system_prompt: |
      You are an expert coding assistant. Provide code examples and explanations. Be precise.
    temperature: 0.2
    retrieve_documents: false
    memory_profile: short_term_minimal

  research:
    system_prompt: |
      You are a professional research assistant. Provide detailed, well-sourced explanations.
    temperature: 0.3
    retrieve_documents: true
    memory_profile: rolling_summary_heavy
